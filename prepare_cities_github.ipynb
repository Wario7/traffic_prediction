{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell requests PeMS traffic data.\n",
    "\n",
    "### Algorithm ###\n",
    "# login() with your PeMS username and password\n",
    "# prepare_response reads station_ids.csv in folder dataset/'city'\n",
    "# prepare_response goes through each station in station_ids.csv\n",
    "# prepare_response reads 8 weeks of vehicle 'flow' via get_response()\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from xml.etree import ElementTree\n",
    "from networkx.readwrite import gpickle\n",
    "from dateutil.parser import parse\n",
    "from datetime import *; \n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "def read_response(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        response = pickle.load(f)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def prepare_response(session):\n",
    "    # root = \"dataset\\\\anaheim\\\\\"\n",
    "    root = \"dataset\\\\oakland\\\\\"\n",
    "    df = pd.read_csv(root + 'station_ids.csv', header=0)\n",
    "    ids = df['ID'].values\n",
    "\n",
    "    weeks_to_read = 8 # weeks\n",
    "    for i in ids:\n",
    "        station_response = ''\n",
    "        \n",
    "        # check if station response was written before\n",
    "        if(os.path.isfile(root + 'raw_data_2_months\\\\' + str(i) + '.txt')):\n",
    "            print(i, '.txt exists in destination, skipping..')\n",
    "            continue\n",
    "        \n",
    "        # get 8 weeks of response\n",
    "        for w in range(weeks_to_read):\n",
    "            start_time = '2016-01-01'\n",
    "            start_time = parse(start_time)\n",
    "            start_time = start_time + relativedelta(weeks=+w)\n",
    "            start_time = start_time + relativedelta(hours=+3) # synching with server\n",
    "            end_time = start_time + relativedelta(weeks=+1, minutes=-1)\n",
    "\n",
    "            s_time_id = str(int(start_time.timestamp()))\n",
    "            s_time_id_f = start_time.strftime('%m') + '%2F' + start_time.strftime('%d') + '%2F2016+00%3A00' \n",
    "            e_time_id = str(int(end_time.timestamp()))\n",
    "            e_time_id_f = end_time.strftime('%m') + '%2F' + end_time.strftime('%d') + '%2F2016+23%3A59'\n",
    "\n",
    "            string = get_response(session=s, station_id=str(i), s_time_id=s_time_id, s_time_id_f=s_time_id_f, e_time_id=e_time_id, e_time_id_f=e_time_id_f)\n",
    "\n",
    "            if(i is not 0):\n",
    "                string = string.split('\\n', maxsplit=1)[1]\n",
    "\n",
    "            station_response = station_response + string\n",
    "        \n",
    "        # write response\n",
    "        with open(root + 'raw_data_2_months\\\\' + str(i) + \".txt\", \"w\") as text_file:\n",
    "                print(\"{}\".format(station_response), file=text_file)\n",
    "        \n",
    "        \n",
    "\n",
    "    print('Done.')\n",
    "    return station_response\n",
    "\n",
    "    \n",
    "def get_response(session, station_id, s_time_id, s_time_id_f, e_time_id, e_time_id_f):\n",
    "    try:\n",
    "        print('station_id:', station_id)\n",
    "        url = 'http://pems.dot.ca.gov/?report_form=1&dnode=VDS&content=loops&tab=det_timeseries&export=text' \\\n",
    "            + '&station_id=' + station_id \\\n",
    "            + '&s_time_id=' + s_time_id \\\n",
    "            + '&s_time_id_f=' + s_time_id_f \\\n",
    "            + '&e_time_id=' + e_time_id \\\n",
    "            + '&e_time_id_f=' + e_time_id_f \\\n",
    "            + '&tod=all&tod_from=0&tod_to=0&dow_0=on&dow_1=on&dow_2=on&dow_3=on&dow_4=on&dow_5=on&dow_6=on' \\\n",
    "            + '&holidays=on' \\\n",
    "            + '&q=flow' \\\n",
    "            + '&q2=&gn=5min' \\\n",
    "            + '&agg=on' \\\n",
    "            + '&lane1=on'\n",
    "        \n",
    "        print(url)\n",
    "        response = session.post(url)\n",
    "        response.raise_for_status\n",
    "        # root = ElementTree.fromstring(response.content)\n",
    "        # string = ElementTree.tostring(root, encoding='utf8', method='xml')\n",
    "        \n",
    "        \n",
    "        string = response.content.decode('utf-8')\n",
    "        return string\n",
    "    except(Exception):\n",
    "        print('station_id:', station_id)\n",
    "        return ''\n",
    "    \n",
    "def login():\n",
    "    # replace username with you email registered with PeMS and password.\n",
    "    s = requests.Session()\n",
    "    data = {\"username\":\"\", \"password\":\"\", 'login': 'Login'}\n",
    "    url = \"http://pems.dot.ca.gov/\"\n",
    "    r = s.post(url, data=data)\n",
    "    return s\n",
    "\n",
    "s = login()\n",
    "string = prepare_response(s)\n",
    "s.cookies\n",
    "s.user.is_authenticated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This cell splits raw data into train, val, and test data that acts as an input to a Neural Network Model\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as sp\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import floor\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras import backend as K\n",
    "from numpy import genfromtxt\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "def df_split(df, val_size=0.10, test_size=0.20):  \n",
    "    time_interval = 5\n",
    "    N = (24 * 60) // time_interval\n",
    "    days = df.shape[0] // N\n",
    "    days_train = floor(days * 0.7)\n",
    "    days_validate = floor(days * val_size)\n",
    "    days_test = days - days_train - days_validate\n",
    "    \n",
    "    df_train = df[0:days_train*N]\n",
    "    df_val = df[days_train*N:(days_train+days_validate)*N]\n",
    "    df_test = df[(days_train+days_validate)*N:]\n",
    "    \n",
    "    print('days_train:', days_train)\n",
    "    print('days_validate:', days_validate)\n",
    "    print('days_test:', days_test)\n",
    "        \n",
    "    return [df_train, df_val, df_test]\n",
    "\n",
    "# fills missing values with average of the same hour and minute\n",
    "def fillaverage(df):\n",
    "    for h in range(24):\n",
    "        for m in range(0, 60, 5):\n",
    "            hourminaverage = df[(df.index.hour == h) & (df.index.minute == m)].mean().vehicleCount\n",
    "            hourminaverage = math.floor(hourminaverage)\n",
    "            df['vehicleCount'][(df.index.hour == h) & (df.index.minute == m)] = df['vehicleCount'][(df.index.hour == h) & (df.index.minute == m)].fillna(hourminaverage)\n",
    "    return df\n",
    "\n",
    "def get_samples(input_dim, output_dim, predict_range, df):\n",
    "    five_minutes_in_a_day = 1440 // 5\n",
    "    number_of_days = df.shape[0] // five_minutes_in_a_day\n",
    "    samples_per_day = five_minutes_in_a_day - input_dim - predict_range + 1\n",
    "\n",
    "    inputdata = np.zeros((samples_per_day * number_of_days, input_dim))\n",
    "    outputdata = np.zeros((samples_per_day * number_of_days, output_dim))\n",
    "    \n",
    "    for d in range(number_of_days):\n",
    "        for i in range(samples_per_day):\n",
    "            sample_index = i + (d * five_minutes_in_a_day)\n",
    "            input_index = i + (d * samples_per_day)\n",
    "            x = df[sample_index:sample_index+input_dim]\n",
    "            inputdata[input_index, :] = x\n",
    "            # outputdata[input_index] = df[sample_index+input_dim]\n",
    "            # outputdata[sample_index] = df[sample_index+input_dim:sample_index+input_dim+output]\n",
    "            # y = [df[sample_index+input_dim], df[sample_index+input_dim+2], df[sample_index+input_dim+5]]\n",
    "            y = [df[sample_index+input_dim+predict_range-1]]\n",
    "            outputdata[input_index, :] = y\n",
    "        \n",
    "    # outputdata = outputdata.flatten()\n",
    "    return inputdata, outputdata\n",
    "\n",
    "def clean_data(weekdays=False, weekends=False):\n",
    "    # outputs = ['t24', 't36']\n",
    "    # predict_range = [24, 36]\n",
    "    # input_dim = [24, 36]\n",
    "    \n",
    "    outputs = ['t1']\n",
    "    predict_range = [1]\n",
    "    input_dim = [12]\n",
    "    cities = ['oakland']\n",
    "    \n",
    "    for o, p, idim in zip(outputs, predict_range, input_dim):\n",
    "        # input_dim = 12\n",
    "        output_dim = 1\n",
    "        \n",
    "        root = \"dataset\\\\\"+ cities[0] + \"\\\\raw_data_month\\\\\"\n",
    "        train_destination = \"dataset\\\\\"+ cities[0] + \"\\\\clean_data\\\\weekdays_\" + str(idim) + 'i_' + o + \"\\\\train\\\\\"\n",
    "        val_destination = \"dataset\\\\\"+ cities[0] + \"\\\\clean_data\\\\weekdays_\" + str(idim) + 'i_' + o + \"\\\\val\\\\\"\n",
    "        test_destination = \"dataset\\\\\"+ cities[0] + \"\\\\clean_data\\\\weekdays_\" + str(idim) + 'i_' + o + \"\\\\test\\\\\"\n",
    "        files = os.listdir(root)\n",
    "\n",
    "        for i in files:\n",
    "            print(root + i)\n",
    "\n",
    "            if(os.path.exists(train_destination + i + '.npz')):\n",
    "                print(i, 'exists in destination, skipping..')\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(root + i, parse_dates=[0], header=None, index_col=0, sep='\\t')\n",
    "            except:\n",
    "                print('skipped empty df:', i)\n",
    "                continue\n",
    "\n",
    "            if(df.empty):\n",
    "                print('skipped empty df:', i)\n",
    "                continue\n",
    "            \n",
    "            print(len(df))\n",
    "            if(len(df) != (288 * 7 * 4)):\n",
    "                print('skipped short df:', i)\n",
    "                continue\n",
    "            \n",
    "            if(weekdays):\n",
    "                df = df[df.index.dayofweek < 5]\n",
    "            if(weekends):\n",
    "                df = df[df.index.dayofweek >= 5]\n",
    "\n",
    "            # avgSpeed = df['avgSpeed'].astype(np.float64).values\n",
    "\n",
    "            if(df.dtypes[2] != np.float64):\n",
    "                df[2] = df[2].map(lambda x: x.replace(',', ''))\n",
    "                df[2] = df[2].astype(np.float64)\n",
    "\n",
    "            vehicleCount = df[2].astype(np.float64).values\n",
    "\n",
    "            [df_train, df_val, df_test] = df_split(vehicleCount)\n",
    "\n",
    "            if((df_train.size == 0) or (df_val.size == 0) or (df_test.size == 0)):\n",
    "                print('skipped empty df after split:', i)\n",
    "\n",
    "            x_train, y_train = get_samples(idim, output_dim, p, df_train)\n",
    "            x_val, y_val = get_samples(idim, output_dim, p, df_val)\n",
    "            x_test, y_test = get_samples(idim, output_dim, p, df_test)\n",
    "\n",
    "            # clean_df = np.concatenate((df_train, df_val, df_test))\n",
    "            np.savez(train_destination + i, x_train, y_train)\n",
    "            np.savez(val_destination + i, x_val, y_val)\n",
    "            np.savez(test_destination + i, x_test, y_test)\n",
    "\n",
    "    print('done')\n",
    "\n",
    "def normalize(x):\n",
    "    std = x.std()\n",
    "    if(std == 0):\n",
    "        return (x - x.mean()) / np.finfo(np.float64).min\n",
    "    if(np.isnan(std)):\n",
    "        return (x - x.mean()) / np.finfo(np.float64).max\n",
    "    return (x - x.mean()) / std\n",
    "    # return (x - min(x)) / (max(x) - min(x))\n",
    "    \n",
    "clean_data(weekdays=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell prepares the input for a clustering algorithm\n",
    "\n",
    "def prepare_clusters(city, weekdays=False, weekends=False):\n",
    "    x = []\n",
    "    root = \"dataset\\\\\" + city + \"\\\\raw_data_month\\\\\"\n",
    "    files = os.listdir(root)\n",
    "    \n",
    "    for i in files:\n",
    "        print(root + i)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(root + i, header=None, parse_dates=[0], index_col=0, sep='\\t')\n",
    "        except:\n",
    "            print('skipped empty df:', i)\n",
    "            continue\n",
    "        \n",
    "        if(df.empty):\n",
    "            print('skipped empty df:', i)\n",
    "            continue\n",
    "        \n",
    "        print(len(df))\n",
    "        if(len(df) != (288 * 7 * 4)):\n",
    "            print('skipped short df:', i)\n",
    "            continue\n",
    "        \n",
    "        if(weekdays):\n",
    "            df = df[df.index.dayofweek < 5]\n",
    "        if(weekends):\n",
    "            df = df[df.index.dayofweek >= 5]\n",
    "        \n",
    "        if(df.dtypes[2] != np.float64):\n",
    "            df[2] = df[2].map(lambda x: x.replace(',', ''))\n",
    "            df[2] = df[2].astype(np.float64)\n",
    "\n",
    "        t = np.mean(df[2].astype(np.float64).values)\n",
    "        x.append([i, [t]])\n",
    "        \n",
    "    return x\n",
    "\n",
    "x = prepare_clusters(city=\"anaheim\", weekdays=True)\n",
    "ordered = list(f[0] for f in x)\n",
    "kmeans_input = list(f[1] for f in x)\n",
    "kmeans_input = np.array(kmeans_input)\n",
    "kmeans_input = np.vstack(kmeans_input)\n",
    "print(kmeans_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 2 0 0 2 2 0 2 1 1 0 0 0 2 0 1 0 0\n",
      " 2 0 0 2 1 1 0 0 1 0 0 2 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 0 2 0\n",
      " 0 1 0 2 0 0 2 0 2 2 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 1 1 1\n",
      " 0 1 0 0 1 0 1 1 1 0 1 2 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 1 0 0 2 1 0 0 1 1 0\n",
      " 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0\n",
      " 1 0 2 0 0 0 0 0 2 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 2 0 1 0 1 0 2 0 0 0 0 0 0 0 0 0 2 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[  41.98705552]\n",
      " [ 376.4614945 ]\n",
      " [ 259.17594763]]\n",
      "[255 201  24]\n"
     ]
    }
   ],
   "source": [
    "# This cell uses the output of the previous cell as an input to the clustering algorithm\n",
    "# The clustering algorithm seggregates raw data into 3 groups: low, medium, high indicated by 0, 1, 2 respectively\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# K-means\n",
    "city = 'anaheim'\n",
    "\n",
    "def map_traffic(x):\n",
    "    if(x == 0):\n",
    "        return 'medium'\n",
    "    if(x == 1):\n",
    "        return 'low'\n",
    "    if(x == 2):\n",
    "        return 'high'\n",
    "    \n",
    "# def get_clusters():\n",
    "#     filepath = \"AarhusClusters.csv\"\n",
    "#     df = pd.read_csv(filepath, index_col=0)\n",
    "#     df.index = df.index - 1\n",
    "\n",
    "#     high = df[df['TrafficLevelAvg'] == 'high'].index\n",
    "#     medium = df[df['TrafficLevelAvg'] == 'medium'].index\n",
    "#     low = df[df['TrafficLevelAvg'] == 'low'].index\n",
    "        \n",
    "#     return low, medium, high\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(kmeans_input)\n",
    "print(kmeans.labels_)\n",
    "print(kmeans.cluster_centers_)\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, kmeans_input)\n",
    "print(closest)\n",
    "\n",
    "labels = np.array(list(map(map_traffic, kmeans.labels_)))\n",
    "file_labels = np.column_stack((ordered, labels))\n",
    "\n",
    "output = pd.DataFrame(file_labels)\n",
    "output.to_csv('dataset\\\\'+ city +'\\\\file_labels.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell prints the number of elements in low, medium, and high clusters\n",
    "\n",
    "def get_clusters(city, random=False):\n",
    "\n",
    "    filepath = \"dataset\\\\\" + city + \"\\\\file_labels.csv\"\n",
    "    df = pd.read_csv(filepath, index_col=0, header=None, names=['file_name', 'TrafficLevelAvg'])\n",
    "    if(random):\n",
    "        print('--USING RANDOM CLUSTERS--')\n",
    "        indexes = np.arange(len(df.index))\n",
    "        np.random.shuffle(indexes)\n",
    "        segmentation_size = len(indexes) // 3\n",
    "        # low = indexes[:63]\n",
    "        # medium = indexes[63:269]\n",
    "        # high = indexes[269:]\n",
    "        low, medium, high = np.split(indexes, [segmentation_size, segmentation_size*2])\n",
    "        print(len(low), len(medium), len(high))\n",
    "    else:\n",
    "        # high = df[df['TrafficLevelAvg'] == 'high'].index\n",
    "        # medium = df[df['TrafficLevelAvg'] == 'medium'].index\n",
    "        # low = df[df['TrafficLevelAvg'] == 'low'].index\n",
    "        \n",
    "        high = df[df['TrafficLevelAvg'] == 'high'].index\n",
    "        medium = df[df['TrafficLevelAvg'] == 'medium'].index\n",
    "        low = df[df['TrafficLevelAvg'] == 'low'].index\n",
    "        \n",
    "    return low, medium, high\n",
    "\n",
    "low, medium, high = get_clusters(city='oakland')\n",
    "print('low:', len(low))\n",
    "print('medium:', len(medium))\n",
    "print('high:', len(high))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
